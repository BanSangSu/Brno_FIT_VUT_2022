{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BISSIT2022_Audio_exercise.ipynb","provenance":[{"file_id":"1jq_b8ssnj3uIJKWYuiw68KTyDZ_03-bH","timestamp":1658167001425},{"file_id":"1OzncSGbGxIadh-3eB8wgoXRI0iRfT8Tu","timestamp":1658165991533}],"collapsed_sections":[],"authorship_tag":"ABX9TyNf4GubU9ebRNs6oXxQhNu2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hnwnhTKMZoHj"},"source":["# Audio and speech processing - Exercise\n","**Author: Katerina Zmolikova, izmolikova@fit.vutbr.cz**\n","\n","In this exercise, you will work with audio data, learn to plot and listen to them, and do a simple speaker verification with a pre-trained model. There are several places in the notebook, where you are asked to fill in your own code. These are:\n","1. Recording and plotting your own signals.\n","2. Scoring set of target and non-target trials with speaker verification model.\n","3. Listening to the trials, which were classified the worst.\n","4. Running speaker verification with your own recording.\n","\n","**Before you start filling the exercise, create a copy of this notebook, download the necessary \n","[files](https://drive.google.com/file/d/1DRe9732g7eM3TdqU1t1j3E9vHrYiznKh/view?usp=sharing), unzip them and place them to your Colab Notebook (Folder icon on the left > File icon with the up arrow).** This should result in having a text files `trial` and a directory `wavs` with 15 wav files inside."]},{"cell_type":"code","metadata":{"id":"9LGK_UDMaENN"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import soundfile as sf # For loading audio signals into numpy arrays.\n","import IPython.display as dsp # For playing the audio.\n","from pathlib import Path # For working with the file system.\n","import os\n","from scipy.spatial.distance import cosine # For computing cosine distance."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qgyq7TTK4Flc"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3CLmVW42egt"},"source":["!pip install speechbrain"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaQDdJcE2kdx"},"source":["import torchaudio # For loading audio signals directly into torch tensors.\n","from speechbrain.pretrained import EncoderClassifier # For loading pre-trained speaker verification model."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ravLOdjJ5kUI"},"source":["# Loading and plotting signal\n","\n","In the first part of the assignment, we will look at how to plot audio recordings, their spectrograms and how to listen to them directly in the notebook. We will use one of the provided wav files, which was taken from [Voxceleb dataset](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/). First, let's simply read the signal, using `soundfile` library. The command `sf.read` will return two variables:\n","* the signal itself as a `numpy` array.\n","* sampling frequency --- this denotes how many samples per second the wav file has. Here, we will work with signal with 16000 Hz sampling frequency (16000 samples per second)."]},{"cell_type":"code","metadata":{"id":"QEfZuIsvaMYq"},"source":["s,fs = sf.read('wavs/id01224_irx71tYyI-Q.wav')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhpvTLkYbd4r"},"source":["print(f'Sampling frequency is {fs}.')\n","print(f'Number of samples in the signal is {s.shape[0]}, corresponding to {s.shape[0] / fs} seconds.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"22ilhwVREPFo"},"source":["Next, we will plot the loaded signal and play it."]},{"cell_type":"code","metadata":{"id":"8mV5JUYLb9Hq"},"source":["plt.figure(figsize=(8,3))\n","t = np.arange(s.shape[0]) / fs # time axis\n","plt.plot(t, s)\n","plt.gca().set_xlabel('Time [s]')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFnHH-kP5A7b"},"source":["dsp.display(dsp.Audio(s, rate=fs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KxJvEH_IEfGh"},"source":["Next, let's plot a short (100 millisecond long) segment from the signal. When indexing the numpy array, we need to index in samples. Notice that in this part of the signal, we can see a periodic behavior. The period seems to repeat about 3 times per 0.02 seconds, which corresponds to 150 times per second. This means that the base frequency of the voice of the speaker is about 150 Hz. This is the frequency with which the vocal folds vibrate."]},{"cell_type":"code","metadata":{"id":"MYnhQIrh3ndM"},"source":["from_sec = 1.3 # beginning of the segment in seconds\n","to_sec = 1.4 # end of the segment in seconds\n","from_sample = int(from_sec * fs) # beginning of the segment in samples\n","to_sample = int(to_sec * fs) # end of the segment in samples\n","\n","plt.figure(figsize=(8,3))\n","plt.plot(t[from_sample:to_sample], s[from_sample:to_sample])\n","plt.gca().set_xlabel('Time [s]')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk8zzHzaFcPS"},"source":["Finally, we will plot the spectrogram, showing us the time-frequency behavior of the signal. At some parts (e.g. right before 0.5 second mark) you can notice repeating lines at multiplies of the base frequency (150 Hz). These are the voiced parts of the speech."]},{"cell_type":"code","metadata":{"id":"MmRqfUxC5Dty"},"source":["plt.figure(figsize=(15,3))\n","t = np.arange(s.shape[0]) / fs # time axis\n","plt.specgram(s, Fs=fs)\n","plt.gca().set_xlabel('Time [s]')\n","plt.gca().set_ylabel('Frequency [Hz]')\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9_GX_5i5tlt"},"source":["# Loading and plotting your own signals"]},{"cell_type":"markdown","metadata":{"id":"fe2hl6Fa58oT"},"source":["In this part, you will read and plot your own recorded signals. For a guide on how to record in the proper format, please see the provided Google Doc \"How to record audio in given format\". After recording, upload your files to the notebook (folder icon on the left, then upload icon). \n","\n","Your task has three parts explained below. All commands that you need to complete the tasks should be in the section above.\n","\n","1.   Record three signals:\n","*   First one: talking for about 5 seconds (you can for example read a piece of text or just talk spontaneously).\n","*   Second one: saying 'aaaaaaaaaa' (or a different vowel).\n","*   Third one: saying 'sssssssssss'.\n","\n","Record the signal into a WAV file with sampling frequency 16000 Hz, sample size 16 bit and signed-integer encoding. \n","2.   Plot signal and spectrogram of the first one of the recordings (you talking for 5 seconds).\n","3.   Plot a small segment (about 100 miliseconds) of the second and thrid recording ('aaaaaa' and 'ssssss'). On the recording with 'aaaaaaa' you should see the periodic behavior. Estimate how many times per second the period repeats. This is the frequency with which your vocal folds vibrate.\n","\n"]},{"cell_type":"code","metadata":{"id":"OfHKHuDR76pW"},"source":["# Placeholder for your code\n","# Loading your signals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jdF_XKrf7-gk"},"source":["# Placeholder for your code\n","# Plotting the first signal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lh7_Pauw8Bzf"},"source":["# Placeholder for your code\n","# Plotting the spectrogram of the first signal."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G2YjKHtJ8FeM"},"source":["# Placeholder for your code\n","# Plotting 100 milisecond segment of 'aaaaaa'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8HPR2BG8MT4"},"source":["# Placeholder for your code\n","# Plotting 100 milisecond segment of 'ssssss'."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sBLMrV5S8b_g"},"source":["# Speaker verification"]},{"cell_type":"markdown","metadata":{"id":"wBo3Ej-QnBWV"},"source":["In this part of the assignment, we will use a speaker verification model to extract speaker embeddings from several recordings and compute distances between the extracted embeddings. First, we will load a pre-trained speaker verification model. This model was trained on [VoxCeleb](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/) data. You can find more information about the model [here](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)."]},{"cell_type":"code","metadata":{"id":"yfgCBnRFdhnL"},"source":["classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isD0UHfYnvaM"},"source":["With the model, we will score speaker verification trials. This means given a pair of recordings, we want to decide whether the speech at the recordings was spoken by the same speaker or two different speakers. To do this, we will extract a speaker embedding from each of the two recordings and then get a score by computing cosine distance between the embeddings. When the two recordings are of the same speaker, the score should be lower, while when they are from two different speakers, the score should be higher.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Iga7YIYVpiFQ"},"source":["First, we load a text file with definition of the trials. Each line in this file is as follows:\n","\n","`[name-of-first-recording] [name-of-second-recording] [target/nontarget]`\n","\n","When the last column is `target`, both recording are from the same speaker. When the last column is `nontarget`, each recording is from different speaker. The recordings, including their speaker labels,  are taken from VoxCeleb test set. \n","\n","We will load the trials into two lists, one for target, one for non-target. Each element of the list is a tuple containing names of both files."]},{"cell_type":"code","metadata":{"id":"5B9_b1Zu2V61"},"source":["with open('trials') as f:\n","  trials = [line.strip().split() for line in f]\n","trials_target = [(trial[0],trial[1]) for trial in trials if trial[2] == 'target']\n","trials_nontarget = [(trial[0],trial[1]) for trial in trials if trial[2] == 'nontarget']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_IhSgQM9gtM"},"source":["trials_target[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldrYodfvqT3h"},"source":["We will first inspect the first target trial. We load and play both of the recordings. Notice that here, we use `torchaudio.load` rather than `sf.read` for reading the audio files. `torchaudio.load` loads the files directly into torch tensor, which is convenient for using it as the input of our model. However, when we want to plot or play the loaded signals, we need to use `.numpy()` to convert them to numpy array."]},{"cell_type":"code","metadata":{"id":"HGKL50XZ9qJM"},"source":["s1, fs = torchaudio.load(f'wavs/{trials_target[0][0]}')\n","s2, fs = torchaudio.load(f'wavs/{trials_target[0][1]}')\n","\n","dsp.display(dsp.Audio(s1.numpy(), rate=fs))\n","dsp.display(dsp.Audio(s2.numpy(), rate=fs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qEoWe0YqblC"},"source":["We extract the embeddings from both loaded signals. The model returns the embedding with shape `(1,1,512)`. We are interested only in the last dimension, which is the dimension of the embedding itself. "]},{"cell_type":"code","metadata":{"id":"nZgUYUON-ERE"},"source":["emb1 = classifier.encode_batch(s1)[0][0]\n","emb2 = classifier.encode_batch(s2)[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lrie8n2qtMj"},"source":["Finally, we compute the cosine distance between the two embeddings."]},{"cell_type":"code","metadata":{"id":"mS2jNjqe-WSc"},"source":["dist = cosine(emb1, emb2)\n","print(f'Distance of the speaker embeddings (the score of the trial) is {dist}.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJPjE8hQq0p_"},"source":["**Here comes your task:**\n","Extract the scores (cosine distances) for all target and non-target trials. For this, you need to load the audio from the files, extract the speaker embeddings and compute the cosine distance between them. Store the target trial scores in list `scores_target` and non-target scores in list `scores_nontarget`. "]},{"cell_type":"code","metadata":{"id":"8ezvWEJM2fsB"},"source":["scores_target = []\n","\n","for u1, u2 in trials_target:\n","  # Your code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bR_vnHGd4Xy1"},"source":["scores_nontarget = []\n","\n","for u1, u2 in trials_nontarget:\n","  # Your code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KTgyDW3SrOXR"},"source":["Now, we can plot a histogram of all scores. For this, we use `matplotlib` command `plt.hist`, with additional parameters:\n","* `color` - we use green for target trials and red for non-target\n","* `alpha` - setting transparency of the histograms, so that we nicely see the overlapping parts\n","* `range` - controlling the lowest and highest value of which the histograms are computed. Change if you use the same on different recordings and get scores in different interval.\n","* `bins` - number of bins of the histogram.\n","\n","In the histogram, you should see target green scores to be lower than non-target red scores, with some overlap. \n","\n","By looking at the histogram, decide on score which you would use as the threshold. Everything higher than this threshold is classified as non-target. Everything lower than this threshold is classified as target. (There are no right or wrong answers here. Simply look at the scores and think about how the choice of threshold affects the type of errors that you would get and how in different applications, you could set the threshold differently.)"]},{"cell_type":"code","metadata":{"id":"DaUXkWs70CFM"},"source":["plt.figure()\n","plt.hist(scores_target, color='green', alpha=0.5, range=[0,0.2], bins=50)\n","plt.hist(scores_nontarget, color='red', alpha=0.5, range=[0,0.2], bins=50)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iuGLVBwlsvIQ"},"source":["From the histogram, you can see that any threshold you choose, there will be always some recordings classified wrongly. Let's inspect the recordings with the worst scores - this means target trial with highest score and non-target trial with lowest score.\n","\n","Your task here, is to load and play the recordings from the worst trials (two from the worst target trial, and two from the worst non-target trial). Do the speaker in the target trial sound similar to you? Do the speakers from the non-target trial sound different to you? Does it make sense that our model made a mistake here?"]},{"cell_type":"code","metadata":{"id":"tRy0m7FI0IsN"},"source":["trials_target[np.argmax(scores_target)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ouyC_0i-6nH"},"source":["# Placeholder for you code\n","# Play the two signals from the worst target-trial. Does it sound like the same speaker in both?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztF45ufh0WkL"},"source":["trials_nontarget[np.argmin(scores_nontarget)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlcCHIJ154nA"},"source":["# Placeholder for you code\n","# Play the two signals from the worst nontarget-trial. Does it sound like different speakers?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d6W2Ks4G_UDC"},"source":["# Speaker verification with your own recording"]},{"cell_type":"markdown","metadata":{"id":"H64JDWuythxh"},"source":["In the last part of the assignment, you will use your own recording and compare it with all the recordings from the `wavs` directory. For this, use the recording of you talking freely for five seconds. The solution should follow the same logic as above with trials being tuples of `(your-recording,recording-from-wav-directory)` --- fifteen trials in total. Ideally, you should see that all trials get higher score than your chosen threshold -> they are classified as different speaker."]},{"cell_type":"code","metadata":{"id":"J3j_bzFQ_YSi"},"source":["threshold = # insert your chosen threshold here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_kMMEjTk_dKH"},"source":["# Placeholder for you code\n","# Get scores comparing your own recording to all recordings from 'wavs' directory.\n","\n","all_wavs = os.listdir('wavs')\n","\n","scores_yours = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNGp2dfR_sP9"},"source":["# How many times was your recording correctly classified as different speaker?\n","np.sum(np.array(scores_yours) > threshold)"],"execution_count":null,"outputs":[]}]}