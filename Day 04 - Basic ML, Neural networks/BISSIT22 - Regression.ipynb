{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BISSIT22 - Regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Download data\n","Competition: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n","\n","Data description: https://www.fit.vutbr.cz/~ikiss/bissit22/nn/data_description.txt"],"metadata":{"id":"u2ZV-acgfIAW"}},{"cell_type":"code","source":["import os\n","\n","if not os.path.isfile(\"data.csv\"):\n","    !wget \"https://www.fit.vutbr.cz/~ikiss/bissit22/nn/data.csv\""],"metadata":{"id":"WVAfHV77cLW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657809765193,"user_tz":-120,"elapsed":5309,"user":{"displayName":"Michal Hradiš","userId":"08639506099308646172"}},"outputId":"5998fd02-2d68-424b-a50a-383f37fdb3b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-07-14 14:42:39--  https://www.fit.vutbr.cz/~ikiss/bissit22/nn/data.csv\n","Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n","Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460676 (450K) [text/csv]\n","Saving to: ‘data.csv’\n","\n","data.csv            100%[===================>] 449.88K   159KB/s    in 2.8s    \n","\n","2022-07-14 14:42:44 (159 KB/s) - ‘data.csv’ saved [460676/460676]\n","\n"]}]},{"cell_type":"markdown","source":["## Parse the data"],"metadata":{"id":"Xw-PYqAofcok"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder, LabelBinarizer, MinMaxScaler, KBinsDiscretizer, FunctionTransformer\n","\n","# Read the data from CSV file\n","data = pd.read_csv(\"data.csv\")\n","df = pd.DataFrame(data)\n","\n","# Function to convert quality to number between 0 and 1\n","# Quality can be 10, 9, 8, ..., 1.\n","def quality_to_number(quality):\n","  return quality/10\n","\n","# Store sale price scaler into variable for later use\n","sale_price_scaler = MinMaxScaler(feature_range=(-1,1))\n","\n","# TODO: Year?\n","\n","# Preprocess data\n","bedrooms = df.BedroomAbvGr.to_numpy().reshape(-1, 1)\n","house_style = LabelBinarizer().fit_transform(df.HouseStyle)\n","central_air = LabelBinarizer().fit_transform(df.CentralAir)  # binary data\n","area = KBinsDiscretizer(n_bins=10, encode=\"onehot-dense\", strategy=\"uniform\").fit_transform(df.LotArea.to_numpy().reshape(-1, 1))  # discretize area into 10 bins\n","overall_quality = FunctionTransformer(func=quality_to_number).fit_transform(df.OverallQual.to_numpy().reshape(-1, 1))\n","sale_price = sale_price_scaler.fit_transform(df.SalePrice.to_numpy().reshape(-1, 1))  # scale to range [-1;1]\n","\n","# Merge input data into a single matrix\n","inputs = np.concatenate((bedrooms, house_style, central_air, area, overall_quality), axis=1)\n","targets = sale_price"],"metadata":{"id":"8UCFWFAP_oJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create datasets"],"metadata":{"id":"VyVQtqqQfkWK"}},{"cell_type":"code","source":["import torch\n","\n","# Initialize the seed with a constant to obtain the same result for every run\n","np.random.seed(42)\n","\n","# Randomly shuffle indices\n","indices = np.arange(inputs.shape[0])\n","np.random.shuffle(indices)\n","\n","# Select train and test indices\n","train_indices = indices[:1200]\n","test_indices = indices[1200:]\n","\n","# Split inputs and targets into train and test subsets\n","train_inputs = torch.from_numpy(inputs[train_indices]).float()\n","train_targets = torch.from_numpy(targets[train_indices]).float()\n","test_inputs = torch.from_numpy(inputs[test_indices]).float()\n","test_targets = torch.from_numpy(targets[test_indices]).float()\n","\n","# Create datasets\n","train_dataset = torch.utils.data.TensorDataset(train_inputs, train_targets)\n","test_dataset = torch.utils.data.TensorDataset(test_inputs, test_targets)"],"metadata":{"id":"TOiKA4obfKdO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define network"],"metadata":{"id":"0C0b1jA-fnji"}},{"cell_type":"code","source":["# Define network as a sequence of layers\n","import torch\n","def create_network():\n","\tnetwork = torch.nn.Sequential(\n","      torch.nn.Linear(21, 128),\n","      torch.nn.ReLU(),\n","      torch.nn.Linear(128, 1)\n","  )\t\t\t\n","\n","\treturn network\n","\n","# Create new model and print its structure\n","model = create_network()\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-RsyRRehsWr","executionInfo":{"status":"ok","timestamp":1657809794914,"user_tz":-120,"elapsed":206,"user":{"displayName":"Michal Hradiš","userId":"08639506099308646172"}},"outputId":"91223b1f-9353-4b22-e372-39dcc8bcfd3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequential(\n","  (0): Linear(in_features=21, out_features=128, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=128, out_features=1, bias=True)\n",")\n"]}]},{"cell_type":"markdown","source":["## Save and load model"],"metadata":{"id":"pq8sdbXbuuMb"}},{"cell_type":"code","source":["import datetime\n","\n","\n","# Creates folders if they don't exist\n","def make_dirs_if_not_exist(path):\n","  if not os.path.isdir(path):\n","    os.makedirs(path)\n","\n","\n","# Saves model to path\n","def save_model(model, path):\n","  torch.save(model.state_dict(), path)\n","\n","\n","# Saves model with timestamp suffix to folder\n","def save_model_timestamp(model, folder=\"checkpoints\"):\n","  make_dirs_if_not_exist(folder)\n","  model_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","  save_model(model, os.path.join(folder, f\"model_{model_time}.pt\"))\n","\n","\n","# Loads model from path\n","def load_model(path):\n","  model = create_network()\n","  model.load_state_dict(torch.load(path))\n","  return model\n","\n","\n","# Loads last model from folder\n","def load_last_model(folder=\"checkpoints\"):\n","  if not os.path.isdir(folder):\n","    return None\n","\n","  models = [file for file in os.listdir(folder) if file.startswith(\"model_\") and file.endswith(\".pt\")]\n","  models = sorted(models)\n","  last_model = models[-1]\n","  return load_model(os.path.join(folder, last_model))"],"metadata":{"id":"AOndCoL7uy3S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training setup"],"metadata":{"id":"GfrLca2Ifq56"}},{"cell_type":"code","source":["# Training setup\n","batch_size = 64\n","view_step = 1000\n","iterations = 10000\n","\n","# Create training and testing DataLoaders\n","training_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n","testing_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, pin_memory=True, num_workers=0)  \n","\n","# Define loss function and optimizer\n","criterion = torch.nn.MSELoss(reduction=\"none\")\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"lSZwtFXMhPtg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training step"],"metadata":{"id":"lQdQHKTGftUa"}},{"cell_type":"code","source":["def training_step(model, input_data, target_labels, criterion, optimizer):\n","\t# Forward pass - compute network autput and store all activations\n","\toutputs = model(input_data)\n","\t\n","\t# Compute loss\n","\tloss = criterion(outputs, target_labels).mean()\n","   \n","\t# Backward pass - compute gradients\n","\toptimizer.zero_grad()\n","\tloss.backward()\n","\n","\t# Optimize network\n","\toptimizer.step()\n","\n"," \t# .item() and .detach() disconects from the computational graph\n","\treturn loss.item(), outputs.detach()"],"metadata":{"id":"ig5_HJg6ifT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing the model"],"metadata":{"id":"on3fxFQ4fxza"}},{"cell_type":"code","source":["def test(model, data_loader, criterion):\n","  model = model.eval()\n","\t\n","  # Accumulators\n","  loss_acc = 0\n","  price_diff_acc = 0\n","  counter = 0\n","\t\n","  # Loop through dataset\n","  for batch_data, batch_labels in data_loader:\n","    # Calculate output\n","    output = model(batch_data)\n","  \n","    # Accumulate loss value\n","    loss_acc += criterion(output, batch_labels).sum().item()\n","  \n","    # Accumulate diff between target and predicted prices\n","    target_price = sale_price_scaler.inverse_transform(batch_labels.detach().numpy())\n","    predicted_price = sale_price_scaler.inverse_transform(output.detach().numpy())\n","    price_diff_acc += np.abs(predicted_price - target_price).sum()\n","\n","    # Accumulate batch size, the last batch might not be the same size as the others therefore we accumulate real size of the data\n","    counter += batch_data.shape[0]\n"," \n","  model = model.train()\n","  return loss_acc / counter, price_diff_acc / counter"],"metadata":{"id":"V2xOyBo4izhr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training"],"metadata":{"id":"Q1lRnkNZf1g6"}},{"cell_type":"code","source":["# Accumulators\n","train_loss_acc = 0\n","train_price_diff_acc = 0\n","iteration = 0\n","stop_training = False\n","\n","# When we reach the end of the dataset, but do not want to end the training, we loop through it again\n","while not stop_training:\n","  # Obtain batch from the training dataset in a loop\n","  for batch_data, batch_labels in training_loader:\n","    iteration += 1\n","\n","    # Do a training step\n","    loss, outputs = training_step(model, batch_data, batch_labels, criterion, optimizer)\n","    \n","    # Accumulate loss for statistics\n","    train_loss_acc += loss\n","\n","    # Calculate diff between target and predicted prices\n","    target_price = sale_price_scaler.inverse_transform(batch_labels.detach().numpy())\n","    predicted_price = sale_price_scaler.inverse_transform(outputs.detach().numpy())\n","    train_price_diff_acc += np.abs(predicted_price - target_price).mean()\n","\n","    # Test model \n","    if iteration % view_step == 0:\n","      # Calculate loss and accuracy on the testing dataset\n","      test_loss_acc, test_price_avg_diff = test(model, testing_loader, criterion)\n","\n","      print(f\"iteration:{iteration} \"\n","            f\"train_loss:{train_loss_acc/view_step:.3f} \"\n","            f\"test_loss:{test_loss_acc:.3f} \"\n","            f\"train_price_avg_diff:{train_price_diff_acc/view_step:.3f} \"\n","            f\"test_price_avg_diff:{test_price_avg_diff:.3f}\")\n","      \n","      # Reset the accumulators\n","      train_loss_acc = 0\n","      train_price_diff_acc = 0\n","\n","    # Stop training when amount of iterations is reached\n","    if iteration >= iterations:\n","      stop_training = True\n","      break\n","\n","print(\"Training finished.\")\n","save_model_timestamp(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"llcOaKKSiH-W","executionInfo":{"status":"ok","timestamp":1657809831222,"user_tz":-120,"elapsed":19107,"user":{"displayName":"Michal Hradiš","userId":"08639506099308646172"}},"outputId":"2822c7b4-6a4f-4d5d-e597-cad427da5528"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["iteration:1000 train_loss:0.022 test_loss:0.015 train_price_avg_diff:35371.872 test_price_avg_diff:32950.924\n","iteration:2000 train_loss:0.014 test_loss:0.014 train_price_avg_diff:29319.927 test_price_avg_diff:30586.674\n","iteration:3000 train_loss:0.013 test_loss:0.014 train_price_avg_diff:28044.886 test_price_avg_diff:29877.316\n","iteration:4000 train_loss:0.012 test_loss:0.014 train_price_avg_diff:27499.168 test_price_avg_diff:29324.996\n","iteration:5000 train_loss:0.012 test_loss:0.014 train_price_avg_diff:26893.395 test_price_avg_diff:30534.263\n","iteration:6000 train_loss:0.012 test_loss:0.014 train_price_avg_diff:26631.117 test_price_avg_diff:29851.211\n","iteration:7000 train_loss:0.011 test_loss:0.016 train_price_avg_diff:26462.967 test_price_avg_diff:33430.770\n","iteration:8000 train_loss:0.011 test_loss:0.015 train_price_avg_diff:26307.376 test_price_avg_diff:31603.045\n","iteration:9000 train_loss:0.011 test_loss:0.015 train_price_avg_diff:26353.930 test_price_avg_diff:30495.985\n","iteration:10000 train_loss:0.011 test_loss:0.015 train_price_avg_diff:26033.059 test_price_avg_diff:30783.336\n","Training finished.\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"ns7UQRNKKjiH"},"execution_count":null,"outputs":[]}]}